{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spped2000/Thai-Handwriting-Recognition/blob/main/HandWritingRecog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvWxirPoR-At"
      },
      "source": [
        "# Thai Letter Classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRaXArxIQ08x"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import glob\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uejI6pvmBTY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YOhjDkgbGXW"
      },
      "source": [
        "# Loading and Preprocessing data\n",
        "filelist = glob.glob(\"/content/drive/MyDrive/Colab Notebooks/DeepLearnin/FaceClassify/Project/set2*.jpg\")\n",
        "img = np.zeros(dtype= 'uint8', shape=(len(filelist), 256, 256, 3))\n",
        "for i in range(len(filelist)):\n",
        "  x = Image.open(filelist[i])\n",
        "  #y = x.resize((256,256))\n",
        "  #z = asarray(y)\n",
        "  #img[i,:,:,:] = z[:,:,:1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WhNIx5Lxwcfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dZdX3x0twcb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7YA-A76QvEeD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7uEGGpJ2gio"
      },
      "source": [
        "# Define Thai letters array\n",
        "# 01 = ก, 02 = ข, ...\n",
        "#letters=u'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮ'\n",
        "classes = ['ก','ข', 'ฃ', 'ค', 'ฅ', 'ฆ','ง','จ','ฉ','ช','ซ','ฌ','ญ','ฎ','ฏ','ฐ','ฑ','ฒ','ณ','ด','ต','ถ','ท','ธ','น','บ','ป','ผ','ฝ','พ','ฟ','ภ','ม','ย','ร','ล','ว','ศ','ษ','ส','ห','ฬ','อ','ฮ']\n",
        "\n",
        "#classes = ['01','02','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','43','44']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUDpXGznU4bI"
      },
      "source": [
        "# Selecting data\n",
        "image_size = (28, 28)\n",
        "batch_size = 32\n",
        "\n",
        "# traning set\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/DeepLearnin/Project/set2\",\n",
        "    validation_split=0.1,\n",
        "    class_names = classes,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "# validation set\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/DeepLearnin/Project/set2\",\n",
        "    validation_split=0.1,\n",
        "    class_names = classes,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5fCzeVcU5VU"
      },
      "source": [
        "# Visulize data\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3w6jqqPU-T8"
      },
      "source": [
        "#data augmentation\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    ])\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn1hUk0RVAxy"
      },
      "source": [
        "# augmented training set\n",
        "augmented_train_ds = train_ds.map(  lambda x, y: (data_augmentation(x, training=True), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmMvfmitVDme"
      },
      "source": [
        "#Configure the dataset for performance\n",
        "train_ds = train_ds.prefetch(buffer_size=batch_size)\n",
        "val_ds = val_ds.prefetch(buffer_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZydYoqxiVEJx"
      },
      "source": [
        "#Build a model\n",
        "num_classes = len(classes)\n",
        "model = Sequential([\n",
        "  data_augmentation,\n",
        "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(28, 28, 3)),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  \n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dropout(0.1), # Dropout \n",
        "  #layers.add(BatchNormalization()) # Batch normalization\n",
        "  # normalizes the inputs heading into the next layer, ensuring that the network always creates activations with the same distribution that we desire\n",
        "  layers.Dense(num_classes),\n",
        "  layers.Activation('softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmfJf6SXVHGd"
      },
      "source": [
        "#Train the model\n",
        "epochs=20\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\"),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P--w2fD9UIVV"
      },
      "source": [
        "#model.save_weights(top_model_weights_path)\n",
        "score = model.evaluate(val_ds, batch_size=batch_size, verbose=1)\n",
        "print(\"score :\", score[0])\n",
        "print(\"accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
        "print(\"Loss: {}\".format(eval_loss)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwQnCHV3UsqP"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqrBv0HRVPqJ"
      },
      "source": [
        "#Graphing our training and validation\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "#plt.save('/content/drive/MyDrive/Colab Notebooks/DeepLearnin/Project/result.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4VmK1DmVUI3"
      },
      "source": [
        "img = keras.preprocessing.image.load_img(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/DeepLearnin/Project/testing_set/t1.jpg', target_size=(28, 28)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(\"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(classes[np.argmax(score)], 100 * np.max(score))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlYaJuKqsg2M"
      },
      "source": [
        "pickle_out = open(\"/content/drive/MyDrive/Colab Notebooks/DeepLearnin/Project/model_trained.p\", \"wb\")\n",
        "pickle.dump(model, pickle_out)\n",
        "picke_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgPUjdP2NTgE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}